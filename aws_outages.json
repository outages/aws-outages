{
  "archive": [
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Network Connectivity in US-GOV-WEST-1",
      "date": "1635874351",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:32 AM PDT</span>&nbsp;We are investigating connectivity issues in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;The issue is affecting network connectivity from the Internet to EC2 instances in a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region, between instances within this Availability Zone and between instances within this Availability Zone and other Availability Zones.</div><div><span class=\"yellowfg\">12:54 PM PDT</span>&nbsp;We have resolved the issue affecting Internet connectivity to a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (US-West)",
      "summary": "[RESOLVED] Network Connectivity",
      "date": "1635875642",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:54 AM PDT</span>&nbsp;Network Connectivity\nWe're investigating network connectivity issues for instances within a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region. We are also seeing increased error rates and latencies for the EC2 APIs within the region and are working to solve the issue.</div><div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue is affecting network connectivity from the Internet to instances in the affected Availability Zone, between instances within the affected Availability Zone and between instances within the affected Availability Zone and other Availability Zones. The EC2 APIs are also experiencing increased error rates and latencies within the US-GOV-WEST-1 Region, which is also affecting the AWS Management Console. We have made some progress in mitigating the impact for other AWS services, such as connectivity to Amazon S3, but continue to work on resolving the issue.</div><div><span class=\"yellowfg\">12:09 PM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. We have identified the root cause of the issue and are now focused on mitigating the issue. We are seeing some recovery in the error rates and latencies for the EC2 APIs and launches of new instances are once again working within the region. For recovery at this stage, we recommend focusing on shifting workloads and traffic away from the affected Availability Zone (USGW-AZ3).</div><div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are seeing recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. Once networking was restored to the affected Availability Zone, affected AWS services are also recovering. While the majority are seeing full recovery, we continue to work on the networking-related EC2 APIs, which are still seeing errors at this stage.</div><div><span class=\"yellowfg\"> 1:10 PM PDT</span>&nbsp;We have seen recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The increased error rates and latencies for the EC2 APIs have also recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-gov-west-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (US-West)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1635879208",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:53 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;Between 10:15 AM and 12:53 PM PDT we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "kinesis-us-gov-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Sao Paulo)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1636421771",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:36 PM PST</span>&nbsp;We are investigating internet connectivity issues within the SA-EAST-1 Region. This is affecting connectivity into the SA-EAST-1 Region but also causing increased error rates and latencies for AWS service APIs within the region.</div><div><span class=\"yellowfg\"> 5:51 PM PST</span>&nbsp;Between 5:15 PM and 5:41 PM PST we experienced Internet connectivity issues for a single Availability Zone (sae1-az1) within the SA-EAST-1 Region. Some AWS services also experienced increased error rates and latencies during this time. The issue has been resolved and the service is operating normally.</div>",
      "service": "internetconnectivity-sa-east-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated API Error Rates",
      "date": "1636495106",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:58 PM PST</span>&nbsp;We are investigating increased 5xx error rates and latencies for requests to the Amazon S3 APIs in the US-EAST-1 Region. Where possible, we recommend that requests that fail with a 5xx error be retried.</div><div><span class=\"yellowfg\"> 2:48 PM PST</span>&nbsp;We have identified the S3 subsystem responsible for increased 5xx error rates for the S3 PUT APIs, and are working to isolate the root cause within this subsystem. Customers may also be experiencing increased latency when performing PUT operations. During this time, we recommend customers retry any failed requests.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;We are continuing to see increased 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. We have narrowed down the root cause to a specific sub-system within S3 and continue to make progress in mitigating the impact to this service but have not yet seen significant improvement. S3 API error rates and latencies have stayed a consistent low level with the vast majority of request retries succeeding. While the vast majority of requests are being processed within the normal latency levels, request tail latencies are exceeding 1 second in some cases. In some applications, increasing client timeouts may also help to mitigate the issue. </div><div><span class=\"yellowfg\"> 4:52 PM PST</span>&nbsp;We are starting to see some improvement in the 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. The issue affected a subsystem that stores routing metadata used by Amazon S3 to map API requests to the storage nodes. A recent update caused increased load within this subsystem, which led to increased error rates and latencies for the S3 APIs. We have now successfully mitigated this increased load within this subsystem and are seeing early signs of recovery. As the sub-system processes the backlog of requests, S3 API error rates and latencies will continue to improve.</div><div><span class=\"yellowfg\"> 5:53 PM PST</span>&nbsp;We continue to see a gradual improvement in error rates as we process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We are currently working on mitigations to speed up the processing of the backlog during this event. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally. </div><div><span class=\"yellowfg\"> 6:58 PM PST</span>&nbsp;We continue to process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We have implemented two parallel mitigations to improve the speed of processing. Both mitigations are in process of deployment. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally.</div><div><span class=\"yellowfg\"> 7:51 PM PST</span>&nbsp;We have completed the mitigation to accelerate processing of the mappings between S3 API request metadata and storage. The backlog has been fully processed and S3 API errors and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-us-standard"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift cluster reboot and degraded performance",
      "date": "1636496591",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:23 PM PST</span>&nbsp;We are investigating cluster reboots and degraded performance for Redshift Clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:47 PM PST</span>&nbsp;We continue to investigate cluster reboots and degraded performance for Redshift RA3 Clusters in the US-EAST-1 Region. The cluster reboots are being triggered by writes that are being impacted by the elevated S3 put latencies. Redshift attempts to retry these writes automatically but if they are unsuccessful after extended attempts, the cluster may restart. If you are able to pause write workloads while we work towards resolving the S3 put latency issue, your clusters will no longer restart and can serve read queries normally.</div><div><span class=\"yellowfg\"> 4:53 PM PST</span>&nbsp;We have identified and continue to work on mitigating the root cause of the Redshift RA3 Clusters reboots and degraded performance in the US-EAST-1 Region. The cluster reboots are triggered by Redshift writes to S3 that have been impacted by the elevated S3 PUT API latencies. If you are impacted and able to pause Redshift write workloads on your RA3 clusters while we work towards recovery, your clusters will no longer restart and can serve read queries normally. As S3 API error rates and latencies continue to improve, we expect Redshift RA3 cluster restarts to decline as well.</div><div><span class=\"yellowfg\"> 7:19 PM PST</span>&nbsp;While the S3 API error rates and latencies continue to hold steady, we continue to see a low rate of Redshift RA3 Cluster restarts. While we continue to take steps to mitigate and reduce the risk of a restart for affected Redshift clusters, we expect to see full recovery when the S3 error rates and latencies have fully recovered. Please refer to the S3 Service Health Dashboard updates for progress towards recovery.</div><div><span class=\"yellowfg\"> 7:47 PM PST</span>&nbsp;As S3 has completed their mitigation and is operating normally, Redshift RA3 clusters are seeing writes succeed and clusters are no longer rebooting. For the vast majority of customers the issue has been resolved and the service is operating normally. We continue to work with a small number of impacted customers individually to recover their clusters and will reach out via the AWS Personal Health Dashboard and AWS Support.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Stockholm)",
      "summary": "[RESOLVED] Increased EC2 Console Error Rates",
      "date": "1637596278",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:51 AM PST</span>&nbsp;We are investigating increased error rates within the EC2 Management Console in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are starting to see recovery for the issue causing increased error rates in the EC2 Console in the EU-NORTH-1 Region. We expect to see full recovery shortly. In the meantime, the EC2 APIs and EC2 CLI remain unaffected by the issue.</div><div><span class=\"yellowfg\"> 8:38 AM PST</span>&nbsp;We have resolved the issue causing increased error rates for the EC2 Management Console in the EU-NORTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-north-1"
    },
    {
      "service_name": "Amazon EventBridge (Cape Town)",
      "summary": "[RESOLVED] Event Delivery Delays",
      "date": "1637641154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating increased event delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:13 PM PST</span>&nbsp;We continue to investigate increased event delivery delays in AF-SOUTH-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:26 PM PST</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken initial mitigation actions based on our investigations and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23, 12:04 AM PST</span>&nbsp;We continue working to identify the root cause of elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken further mitigation actions which have reduced the delivery latency and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23,  1:59 AM PST</span>&nbsp;Between November 22 7:30 PM and November 23 1:32 AM PST, we experienced elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "events-af-south-1"
    },
    {
      "service_name": "Amazon CloudWatch (Cape Town)",
      "summary": "[RESOLVED] Metric Stream Delivery Delays ",
      "date": "1637641175",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating CloudWatch Metric Streams delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:09 PM PST</span>&nbsp;We can confirm elevated API error rates for CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. We are actively working to resolve the issue. Customers may experience issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams is not affected.</div><div><span class=\"yellowfg\"> 9:42 PM PST</span>&nbsp;Between 7:30 PM and 9:25 PM PST, we experienced elevated error rates when calling CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. Customers may have experienced issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams was not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudwatch-af-south-1"
    },
    {
      "service_name": "AWS Lambda (Ohio)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1637779569",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:46 AM PST</span>&nbsp;We are investigating increased invoke error rates in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:10 AM PST</span>&nbsp;We continue to investigate increased function invoke error rates within the US-EAST-2 Region. We are working to determine the root cause of the issue and will continue to provide updates as we make progress in resolving the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">11:34 AM PST</span>&nbsp;We have identified the root cause within Lambda that is causing the increased error rates for function invocations within the US-EAST-2 Region. The subsystem responsible for executing functions is currently impaired and we are working to resolve it. We have seen some improvement in Lambda function invocation error rates and believe that this will continue as we take steps to resolve the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:27 AM and 12:03 PM PST we experienced increased Lambda invoke error rates in the US-EAST-2 Region. It may take some additional time to process backlogged async invoke traffic that accumulated during this period. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-east-2"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1637780142",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:55 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console is the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We are seeing some recovery in error rates and latencies for the AWS Management Console in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PST</span>&nbsp;Between 10:26 AM and 12:05 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon API Gateway (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1637780385",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We are investigating increased API errors for API Gateway in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:24 AM PST</span>&nbsp;We have confirmed that the elevated API error rate is restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs).</div><div><span class=\"yellowfg\">12:20 PM PST</span>&nbsp;Between 10:27 AM and 12:06 PM PST, we experienced elevated API error rate in the US-EAST-2 Region, restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs). The issue has been resolved, and the service is operating normally. </div>",
      "service": "apigateway-us-east-2"
    },
    {
      "service_name": "AWS Batch (Ohio)",
      "summary": "[RESOLVED] Increased API Error Rates and Scaling Delays",
      "date": "1637781298",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:14 AM PST</span>&nbsp;We are investigating increased API error rates and delays in scaling of some AWS Batch Compute Environments in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:25 AM and 12:00 PM PST we experienced increased error rates for all AWS Batch APIs, as well as some Compute Environment scaling delays, in the US-EAST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "batch-us-east-2"
    },
    {
      "service_name": "Amazon Simple Notification Service (Oregon)",
      "summary": "[RESOLVED] Increased Latency and Error Rates",
      "date": "1638021360",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:56 AM PST</span>&nbsp;We are investigating increased latency and error rates for API calls in the US-WEST-2 Region. We will provide more information as we continue to investigate.</div><div><span class=\"yellowfg\"> 6:12 AM PST</span>&nbsp;We are starting to see improved SNS API success rates and latency in the US-WEST-2 Region, and are working towards full recovery.</div><div><span class=\"yellowfg\"> 6:42 AM PST</span>&nbsp;Between 5:05 AM and 5:55 AM PST, we experienced increased SNS API latency and error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-us-west-2"
    },
    {
      "service_name": "Amazon CloudWatch (Ireland)",
      "summary": "[RESOLVED] Delayed CloudWatch Metrics",
      "date": "1638042899",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:55 AM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters.  We are are working towards resolution.</div><div><span class=\"yellowfg\">12:49 PM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters. We have isolated the likely root cause to a subsystem that saw an unexpected jump in resource consumption.  We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 1:52 PM PST</span>&nbsp;We have implemented a fix to address the CloudWatch log event processing delays in the EU-WEST-1 Region and are starting to see signs of recovery. We will provide an update once full recovery has been observed.</div><div><span class=\"yellowfg\"> 2:56 PM PST</span>&nbsp;Between 10:26 AM and 02:40 PM PST, we experienced increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. This was due to a subsystem that saw an unexpected jump in resource consumption. The issue has been resolved and the service is operating normally. New events are processing as normal, while we work through the message backlog. We expect to completely drain the backlog over the next 1 hour.</div>",
      "service": "cloudwatch-eu-west-1"
    },
    {
      "service_name": "AWS CloudShell (N. Virginia)",
      "summary": "[RESOLVED] Increased Latencies and Failure Rates",
      "date": "1638507226",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:53 PM PST</span>&nbsp;We are experiencing increased latencies and failures in launching CloudShell environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:26 PM PST</span>&nbsp;Between 7:05 PM and 9:15 PM PST we experienced increased latencies and failures launching CloudShell environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cloudshell-us-east-1"
    },
    {
      "service_name": "AWS Management Console",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1638894176",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:26 AM PST</span>&nbsp;We are experiencing API and console issues in the US-EAST-1 Region.  We have identified root cause and we are actively working towards recovery.  This issue is affecting the global console landing page, which is also hosted in US-EAST-1.  Customers may be able to access region-specific consoles going to https://console.aws.amazon.com/.  So, to access the US-WEST-2 console, try https://us-west-2.console.aws.amazon.com/</div><div><span class=\"yellowfg\"> 4:25 PM PST</span>&nbsp;We are seeing improvements in the error rates and latencies in the AWS Management Console in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:14 PM PST</span>&nbsp;Between 7:32 AM to 4:56 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "management-console"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1638895771",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:49 AM PST</span>&nbsp;We are experiencing elevated error rates for EC2 APIs in the US-EAST-1 region.  We have identified root cause and we are actively working towards recovery.</div><div><span class=\"yellowfg\"> 3:31 PM PST</span>&nbsp;Between 7:32 AM and 3:10 PM PST we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Connect (N. Virginia)",
      "summary": "[RESOLVED] Degraded Contact Handling",
      "date": "1638896019",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:53 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:08 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 9:18 AM PST</span>&nbsp;We can confirm degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 4:47 PM PST</span>&nbsp;We are seeing improvements to contact handling in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:10 PM PST</span>&nbsp;Between 7:25 AM PST and 4:47 PM PST we experienced degraded Contact handling, increased user login errors, and increased API error rates in the US-EAST-1 Region. During this time, end-customers may have experienced delays or errors when placing a call or starting a chat, and agents may have experienced issues logging in or being connected with end-customers. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "connect-us-east-1"
    },
    {
      "service_name": "Amazon DynamoDB (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1638896271",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:57 AM PST</span>&nbsp;We are currently investigating increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:40 PM PST</span>&nbsp;Between 7:40 AM and 2:25 PM PST, we experienced increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region. Data plane operations were not impacted. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "dynamodb-us-east-1"
    },
    {
      "service_name": "AWS Support Center",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1638896490",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:01 AM PST</span>&nbsp;We are investigating increased error rates for the Support Center console and Support API in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:55 PM PST</span>&nbsp;We continue to see increased error rates for the Support Center console and Support API in the US-EAST-1 Region. Support Cases successfully created via the console or the API may not be successfully routed to Support Engineers. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Between 7:33 AM and 2:25 PM PST, we experienced increased error rates for the Support Center console and Support API in the US-EAST-1 Region. This resulted in errors in creating support cases and delays in routing cases to Support Engineers. The issue has been resolved and our Support Engineering team is responding to cases. The service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "supportcenter"
    },
    {
      "service_name": "Amazon EventBridge (N. Virginia)",
      "summary": "[RESOLVED] Event Delivery Delays",
      "date": "1638917670",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:54 PM PST</span>&nbsp;We have temporarily disabled event deliveries in the US-EAST-1 Region. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will not trigger targets. These events will still be received by EventBridge and will deliver once we recover.</div><div><span class=\"yellowfg\"> 3:00 PM PST</span>&nbsp;We have re-enabled event deliveries in the US-EAST-1 Region, but are experiencing event delivery latencies. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will be delayed.</div><div><span class=\"yellowfg\"> 4:31 PM PST</span>&nbsp;We continue to see event delivery latencies in the US-EAST-1 region. We have identified the root cause and are working toward recovery.</div><div><span class=\"yellowfg\"> 6:00 PM PST</span>&nbsp;Event delivery latency for new events in the US-EAST-1 Region have returned to normal levels. We continue to process a backlog of events.</div><div><span class=\"yellowfg\"> 9:21 PM PST</span>&nbsp;Between 7:30 AM and 8:40 PM PST we experienced elevated event delivery latency in the US-EAST-1 Region. Event delivery latencies have returned to normal levels. Some CloudTrail events for API calls between 7:35 AM and 6:05 PM PST may be delayed but will be delivered in the coming hours.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "events-us-east-1"
    },
    {
      "service_name": "Amazon API Gateway (N. Virginia)",
      "summary": "[RESOLVED] Elevated Errors and Latencies",
      "date": "1638919396",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:23 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 4:05 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are continuing to work towards resolution.</div><div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We have seen improvement in error rates and latencies for invokes in the US-EAST-1 region. We continue to drive towards full recovery.</div><div><span class=\"yellowfg\"> 5:23 PM PST</span>&nbsp;Between 9:02 AM and 5:01 PM PST we experienced increased error rates and latencies for invokes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "apigateway-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Elevated Fargate task launch failures",
      "date": "1638919976",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:32 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day, but we are still investigating task launch failures using the Fargate launch type. Task launches using the EC2 launch type are not impacted.</div><div><span class=\"yellowfg\"> 4:44 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are working towards recovery.</div><div><span class=\"yellowfg\"> 5:31 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are starting to see recovery. As we work towards full recovery, customers may experience insufficient capacity errors and these are being addressed as well.</div><div><span class=\"yellowfg\"> 7:30 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">11:01 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this and have recently seen a decrease in these errors while continuing to work towards full recovery. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">Dec 8,  2:29 AM PST</span>&nbsp;Between 7:31 AM PST on December 7 and 2:20 AM PST on December 8, ECS experienced increased API error rates, latencies, and task launch failures. API error rates and latencies recovered by 6:10 PM PST on December 7. After this point, ECS customers using the EC2 launch type were fully recovered. ECS customers using the Fargate launch type along with EKS customers using Fargate continued to see decreasing impact in the form of insufficient capacity errors between 4:40 PM PST on December 7 and 2:20 AM on December 8. The service is now operating normally. A small set of customers may still experience low levels of insufficient capacity errors and will be notified using the Personal Health Dashboard in that case. There was no impact to running tasks during the event although any ECS task that failed health checks would have been stopped because of that failing health check.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS Batch (N. Virginia)",
      "summary": "[RESOLVED] Increased Job Processing Delays",
      "date": "1638922058",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:07 PM PST</span>&nbsp;We have identified the root cause of increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 5:20 PM PST</span>&nbsp;We have seen improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 8:02 PM PST</span>&nbsp;Improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region is accelerating, we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:29 PM PST</span>&nbsp;Between 7:35 AM and 8:13 PM PST, we experienced increase job state transition delays of AWS Batch Jobs in the US-EAST-1 Region.  The issue has been resolved and the service is now operating normally for new job submissions.  Jobs that were delayed from earlier in the event will be processed in order until we clear the queue.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>",
      "service": "batch-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639055699",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:15 AM PST</span>&nbsp;We are investigating elevated error rates for the Redshift Management Console in the US-EAST-1 region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:34 AM PST</span>&nbsp;We continue to investigate elevated error rates for the Redshift Management Console in the US-EAST-1 region. We have identified the issue causing elevated error rates and are actively working to resolve the issue. Customer clusters remain available and are operating normally. Customers can use API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries. </div><div><span class=\"yellowfg\"> 6:30 AM PST</span>&nbsp;We have identified the root cause of the issue causing elevated error rates and are in the process of deploying a fix that will resolve the issue. We do not have a precise ETA for the deployment to complete that we can share at this time. Customer clusters remain available and are operating normally. Customers can use the API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries.</div><div><span class=\"yellowfg\"> 6:58 AM PST</span>&nbsp;Between 12:00 AM and 6:25 AM PST we saw elevated error rates for the Amazon Redshift Management Console in the US-EAST-1 region. This initial error rate increase was observed at 12:00 AM PST and at 4:05 AM PST this error rate increased further. Customer clusters were operating normally throughout and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. We have completed the deployment of a fix. The issue has been resolved and the Management Console and the Amazon Redshift service are operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Redshift Management Console Errors",
      "date": "1639067113",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:25 AM PST</span>&nbsp;Earlier today we reported elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. At 6:58 AM PST we reported the issue was resolved but we have since detected that some errors persist despite much lower rates. Redshift clusters remain available and customers can manage their clusters using the API and CLI. Customers can also execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:05 AM PST</span>&nbsp;We continue to investigate intermittent elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. Customers will be able to get the console to load if they refresh their browser tab several times. Once the console has loaded, the console will work as expected and customers will be able to execute queries normally. Redshift clusters remain available and customers can manage their clusters using the API and CLI and can execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;Between 7:25 AM and 10:05 AM PST we experienced increased error rates for the Amazon Redshift Management Console in the US-EAST-1 Region. During this time, clusters were operating normally  and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. The issue is resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Oregon)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639582979",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-2 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:14 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-2 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.\n</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-2"
    },
    {
      "service_name": "AWS Internet Connectivity (N. California)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583545",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:52 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-west-1"
    },
    {
      "service_name": "AWS Internet Connectivity (US-West)",
      "summary": "[RESOLVED] Internet Connectivity",
      "date": "1639583722",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:55 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:00 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-GOV-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-GOV-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:16 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>",
      "service": "internetconnectivity-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Console Application Upload Errors",
      "date": "1640158400",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:33 PM PST</span>&nbsp;We are investigating an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. Customers who need to update or deploy a new application version should do so using the AWS CLI. Existing applications are not impacted by this issue</div><div><span class=\"yellowfg\">Dec 22, 12:34 AM PST</span>&nbsp;We continue to investigate an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. We are determining the root causes and working through steps to mitigate the issue. Customers who need to update or deploy a new application version should do so using the AWS CLI while we work towards resolving the issue. Existing applications are not impacted by this issue.</div><div><span class=\"yellowfg\">Dec 22,  1:20 AM PST</span>&nbsp;We have identified the root cause and prepared a fix to address the issue that prevents customers from uploading new application versions through the Elastic Beanstalk console in multiple Regions. The service team is testing this fix and preparing for deployment to the Regions that are affected by this issue. We expect to see full recovery by 3:00 AM PST and will continue to keep you updated if this ETA changes. Customers who need to update or deploy a new application version should do so using the AWS CLI until the issue is fully resolved.</div><div><span class=\"yellowfg\">Dec 22,  3:21 AM PST</span>&nbsp;Between December 21, 2021 at 6:37 PM  and December 22, 2021 at 03:17 AM PST, customers were unable to upload their code through the Elastic Beanstalk console due to a Content Security Policy (CSP) error. Customers were impacted when they attempted to upload a new application version for existing environments or upload their code when creating a new environment in multiple regions. The issue has been resolved and the service is operating normally.</div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] API Error Rates",
      "date": "1640176551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 AM PST</span>&nbsp;We are investigating increased EC2 launch failures and networking connectivity issues for some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 5:01 AM PST</span>&nbsp;We can confirm a loss of power within a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. This is affecting availability and connectivity to EC2 instances that are part of the affected data center within the affected Availability Zone. We are also experiencing elevated RunInstance API error rates for launches within the affected Availability Zone. Connectivity and power to other data centers within the affected Availability Zone, or other Availability Zones within the US-EAST-1 Region are not affected by this issue, but we would recommend failing away from the affected Availability Zone (USE1-AZ4) if you are able to do so. We continue to work to address the issue and restore power within the affected data center.</div><div><span class=\"yellowfg\"> 5:18 AM PST</span>&nbsp;We continue to make progress in restoring power to the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have now restored power to the majority of instances and networking devices within the affected data center and are starting to see some early signs of recovery. Customers experiencing connectivity or instance availability issues within the affected Availability Zone, should start to see some recovery as power is restored to the affected data center. RunInstances API error rates are returning to normal levels and we are working to recover affected EC2 instances and EBS volumes. While we would expect continued improvement over the coming hour, we would still recommend failing away from the Availability Zone if you are able to do so to mitigate this issue.</div><div><span class=\"yellowfg\"> 5:39 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. Network connectivity within the affected Availability Zone has also returned to normal levels. While all services are starting to see meaningful recovery, services which were hosting endpoints within the affected data center - such as single-AZ RDS databases, ElastiCache, etc. - would have seen impact during the event, but are starting to see recovery now. Given the level of recovery, if you have not yet failed away from the affected Availability Zone, you should be starting to see recovery at this stage. </div><div><span class=\"yellowfg\"> 6:13 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. We continue to make progress in recovering the remaining EC2 instances and EBS volumes within the affected Availability Zone. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customers VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 6:51 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. For the remaining EC2 instances, we are experiencing some network connectivity issues, which is slowing down full recovery. We believe we understand why this is the case and are working on a resolution. Once resolved, we expect to see faster recovery for the remaining EC2 instances and EBS volumes. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customers VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 8:02 AM PST</span>&nbsp;Power continues to be stable within the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have been working to resolve the connectivity issues that the remaining EC2 instances and EBS volumes are experiencing in the affected data center, which is part of a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have addressed the connectivity issue for the affected EBS volumes, which are now starting to see further recovery. We continue to work on mitigating the networking impact for EC2 instances within the affected data center, and expect to see further recovery there starting in the next 30 minutes. Since the EC2 APIs have been healthy for some time within the affected Availability Zone, the fastest path to recovery now would be to relaunch affected EC2 instances within the affected Availability Zone or other Availability Zones within the region.</div><div><span class=\"yellowfg\"> 9:28 AM PST</span>&nbsp;We continue to make progress in restoring connectivity to the remaining EC2 instances and EBS volumes. In the last hour, we have restored underlying connectivity to the majority of the remaining EC2 instance and EBS volumes, but are now working through full recovery at the host level. The majority of affected AWS services remain in recovery and we have seen recovery for the majority of single-AZ RDS databases that were affected by the event. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:08 AM PST</span>&nbsp;We continue to make progress in restoring power and connectivity to the remaining EC2 instances and EBS volumes, although recovery of the remaining instances and volumes is taking longer than expected. We believe this is related to the way in which the data center lost power, which has led to failures in the underlying hardware that we are working to recover. While EC2 instances and EBS volumes that have recovered continue to operate normally within the affected data center, we are working to replace hardware components for the recovery of the remaining EC2 instances and EBS volumes. We have multiple engineers working on the underlying hardware failures and expect to see recovery over the next few hours. As is often the case with a loss of power, there may be some hardware that is not recoverable, and so we continue to recommend that you relaunch your EC2 instance, or recreate you EBS volume from a snapshot, if you are able to do so.</div><div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;Over the last hour, after addressing many of the underlying hardware failures, we have seen an accelerated rate of recovery for the affected EC2 instances and EBS volumes. We continue to work on addressing the underlying hardware failures that are preventing the remaining EC2 instances and EBS volumes. For customers that continue to have EC2 instance or EBS volume impairments, relaunching affected EC2 instances or recreating affecting EBS volumes within the affected Availability Zone, continues to be a faster path to full recovery. </div><div><span class=\"yellowfg\"> 1:39 PM PST</span>&nbsp;We continue to make progress in addressing the hardware failures that are delaying recovery of the remaining EC2 instances and EBS volumes. At this stage, if you are still waiting for an EC2 instance or EBS volume to fully recover, we would strongly recommend that you consider relaunching the EC2 instance or recreating the EBS volume from a snapshot. As is often the case with a loss of power, there may be some hardware that is not recoverable, which will prevent us from fully recovering the affected EC2 instances and EBS volumes. We are not quite at that point yet in terms of recovery, but it is unlikely that we will recover all of the small number of remaining EC2 instances and EBS volumes. If you need help in launching new EC2 instances or recreating EBS volumes, please reach out to AWS Support.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Since the last update, we have more than halved the number of affected EC2 instances and EBS volumes and continue to work on the remaining EC2 instances and EBS volumes. The remaining EC2 instances and EBS volumes have all experienced underlying hardware failures due to the nature of the initial power event, which we are working to resolve. We expect to make further progress on this list within the next hour, but some of the remaining EC2 instances and EBS volumes may not be recoverable due to hardware failures. If you have the ability to relaunch an affected EC2 instance or recreate an affected EBS volume from snapshot, we continue to strongly recommend that you take that path.</div><div><span class=\"yellowfg\"> 4:22 PM PST</span>&nbsp;Starting at 4:11 AM PST some EC2 instances and EBS volumes experienced a loss of power in a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Instances in other data centers within the affected Availability Zone, and other Availability Zones within the US-EAST-1 Region were not affected by this event. At 4:55 AM PST, power was restored to EC2 instances and EBS volumes in the affected data center, which allowed the majority of EC2 instances and EBS volumes to recover. However, due to the nature of the power event, some of the underlying hardware experienced failures, which needed to be resolved by engineers within the facility. Engineers worked to recover the remaining EC2 instances and EBS volumes affected by the issue. By 2:30 PM PST, we recovered the vast majority of EC2 instances and EBS volumes. However, some of the affected EC2 instances and EBS volumes were running on hardware that has been affected by the loss of power and is not recoverable. For customers still waiting for recovery of a specific EC2 instance or EBS volume, we recommend that you relaunch the instance or recreate the volume from a snapshot for full recovery. If you need further assistance, please contact AWS Support.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640193970",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 9:26 AM PST</span>&nbsp;We are investigating increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Some customers may begin to see signs of recovery.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">11:56 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">12:10 PM PST</span>&nbsp;As the root cause of this impact is related to Directory Services, we will continue to provide updates on the new post we have just created for Directory Service in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:56 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. The issue has been resolved and the service is operating normally.  If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Directory Service (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD",
      "date": "1640203590",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:06 PM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 2:29 PM PST</span>&nbsp;We continue to resolve increased error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication.  We are prioritizing the most impacted directories to expedite resolution.  Additional customers will see recovery as resolution takes place.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;Our mitigation efforts are working as expected and we are making steady progress toward recovery of error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication. We continue to prioritize the most impacted directories to expedite resolution. Additional customers will see recovery as resolution takes place. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 5:57 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Directory Services in US-EAST-1 Region. This also impacted some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. The issue has been resolved and the service is operating normally. Customers using other Active Directory functionality were not impacted by this issue. If you experience any issues with this service or need further assistance, please contact AWS Support.</div>",
      "service": "directoryservice-us-east-1"
    },
    {
      "service_name": "AWS Internet Connectivity (Mumbai)",
      "summary": "[RESOLVED] Internet connectivity",
      "date": "1640371277",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:41 AM PST</span>&nbsp;Between 8:59 AM and 9:32 AM PST and between 9:40 AM and 10:16 AM PST we observed Internet connectivity issues with a network provider outside of our network in the AP-SOUTH-1 Region. This impacted Internet connectivity from some customer networks to the AP-SOUTH-1 Region. Connectivity between EC2 instances and other AWS services within the Region was not impacted by this event. The issue has been resolved and we continue to work with the external provider to ensure it does not reoccur.\n</div>",
      "service": "internetconnectivity-ap-south-1"
    },
    {
      "service_name": "Amazon Pinpoint (N. Virginia)",
      "summary": "[RESOLVED] Pinpoint Sending/Receiving Delays",
      "date": "1642185596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:39 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of Pinpoint customers sending and receiving SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">10:53 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:42 AM PST</span>&nbsp;We can confirm that the sending and receiving of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while sending and receiving SMS messages using US toll-free numbers. Starting at 5:14 AM SMS message delivery receipts were delayed. These delays will continue while we work with our downstream partners through the backlog of delayed delivery receipts.  The issues have been resolved and the service is operating normally.</div>",
      "service": "pinpoint-us-east-1"
    },
    {
      "service_name": "Amazon Simple Notification Service (N. Virginia)",
      "summary": "[RESOLVED] SMS Delivery Delays",
      "date": "1642186756",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of SNS and Pinpoint customers delivering SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">11:00 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm that the delivery of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:44 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while delivering SMS messages using US toll-free numbers. Also starting at 5:14 AM, SMS message delivery receipts were delayed, which created a backlog of undelivered delivery receipts. We are continuing to work with our downstream partners to clear this backlog. Receipts for new SMS deliveries will also be delayed until this backlog clears. The issues have been resolved and the service is operating normally.</div>",
      "service": "sns-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Increased API Error Rates ",
      "date": "1642432540",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:15 AM PST</span>&nbsp;: AP-NORTHEAST-2  API     . | We are investigating increased API error rates in the ap-northeast-2 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp; (PST)  6 55  7 40  AP-NORTHEAST-2  API  .       .          AWS   (https://console.aws.amazon.com/support)   AWS     . | Between 6:55 AM and 7:40 AM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS Internet Connectivity (Seoul)",
      "summary": "[RESOLVED]   | Network Connectivity",
      "date": "1644150596",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:29 AM PST</span>&nbsp;English follows Korean |    \n\n  (PST)   3 32  3 54 AP-NORTHEAST-2    (apne2-az3)   EC2        .          .     .        .         AWS   (https://console.aws.amazon.com/support)    .\n\nBetween 3:32 AM and 3.54 AM PST we experienced connectivity issues affecting Internet connectivity for some EC2 instances in a single Availability Zone (apne2-az3) in AP-NORTHEAST-2 Region. Connectivity to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored. No action is currently required to address this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",
      "service": "internetconnectivity-ap-northeast-2"
    },
    {
      "service_name": "Amazon Virtual Private Cloud (Frankfurt)",
      "summary": "[RESOLVED] Elevated Error Rates for VPC Console",
      "date": "1646295847",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:24 AM PST</span>&nbsp;We are investigating elevated error rates for the VPC Management Console in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">12:51 AM PST</span>&nbsp;We have identified the root cause and are starting to see recovery of the VPC Management Console in the EU-CENTRAL-1 Region. We recommend signing out and signing back into your account to refresh your session. We continue working towards full recovery and will continue to keep you updated.</div><div><span class=\"yellowfg\"> 1:06 AM PST</span>&nbsp;Between March 2 10:03 PM and March 3 12:30 AM PST, the VPC Management Console experienced elevated error rates in the EU-CENTRAL-1 Region. API and CLI access were not affected. The issue has been resolved and the service is operating normally.</div>",
      "service": "vpc-eu-central-1"
    },
    {
      "service_name": "AWS Lambda (US-West)",
      "summary": "[RESOLVED] Increased API and Invoke Error Rates",
      "date": "1646856563",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;We are investigating increased invoke and API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">12:37 PM PST</span>&nbsp;We can confirm increased invoke and API error rates in the US-GOV-WEST-1 Region. We have deployed a mitigation strategy and continue to work through full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:07 AM and 12:50 PM PST, we experienced increased invoke and API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "lambda-us-gov-west-1"
    },
    {
      "service_name": "Multiple services (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1646859683",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.\n</div><div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are seeing recovery in API error rates for all services.</div><div><span class=\"yellowfg\"> 1:29 PM PST</span>&nbsp;Between 12:43 PM and 12:59 PM PST, we experienced increased error rates and latencies for some AWS services within the US-EAST-1 Region. All services are now operating normally, but S3 Event Notifications continues to process a backlog of events that developed during the event. \n\nThe root cause of this issue was an update to the SQS and Lambda endpoints that inadvertently prevented some traffic from reaching these endpoints.</div><div><span class=\"yellowfg\"> 1:45 PM PST</span>&nbsp;S3 Event Notifications have delivered the backlog of events. This issue is resolved and all services are now operating normally.</div>",
      "service": "multipleservices-us-east-1"
    },
    {
      "service_name": "AWS DataSync (N. Virginia)",
      "summary": "[RESOLVED] Elevated error rates",
      "date": "1647437391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We are investigating elevated error rates for DataSync tasks with EFS source or destination locations resulting in \"The DataSync destination location is not mounted correctly\".</div><div><span class=\"yellowfg\"> 7:00 AM PDT</span>&nbsp;We continue to investigate elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We'll provide an update at 8:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We have identified the cause of the elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We continue to work towards resolution. We'll provide another update at 9:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 9:11 AM PDT</span>&nbsp;We have started to deploy an update to mitigate the elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The deployment will take approximately 1 hour and 45 minutes to reach all affected regions. We will provide another update once the deployment is complete.</div><div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;Between March 15 10:57 PM and March 16 9:57 AM PDT we experienced elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The issue has been resolved and the service is operating normally.</div>",
      "service": "datasync-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Delayed ENI attachment times",
      "date": "1648779488",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We are investigating delayed ENI attachment times for EC2 instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Newly launched EC2 instances or new ENI attachments, may experience delay in establishing network connectivity within the affected Availability Zone. This issue may also affect resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We have identified the root cause and are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;While ENI attachment times have improved, they are still taking longer than normal in the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone. We have identified the root cause for this resource contention and are working to fully resolve the issue. For customers launching instances in the affected Availability Zone or attaching new ENIs to existing instances, full network connectivity on the ENIs may take several minutes to be established, instead of seconds. While we expect attachment times to continue to improve, full recovery here may take up to 2 hours. This issue also affects resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to see an improvement in ENI attachment times, and while they are getting much closer to normal levels, we're still seeing some ENI attachments take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. We are making progress in resolving the resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, and remain on track for full recovery within 1.5 hours. This issue also affects resource provisioning for other services, such as ELB, EMR, ECS and Glue. Some of these services, such as EMR, have mitigated impact by shifting traffic away from the affected Availability Zone, and others like ELB, are now seeing recovery as ENI attachment latencies have improved. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:58 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We remain on track for full recovery within the next hour. At these ENI attachments times, many of the affected services are seeing recovery, or limited impact, as a result of the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 9:31 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We expect to see full recovery within the next 30 to 60 minutes. At these ENI attachments times, ELB and Glue are seeing recovery, while other services - including EMR, EKS, ECS, and RDS - are seeing limited impact within the affected Availability Zone. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:04 PM PDT</span>&nbsp;Over the last 15 minutes, we have seen a further improvement in ENI attachment times within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. The resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone has been resolved, and network state is now beginning to propagate through the affected Availability Zone. As this happens, we would expect to see ENI attachment times return to normal levels over the next 15 - 30 minutes. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:39 PM PDT</span>&nbsp;While we have continued to make some progress over the last 30 minutes, progress has been slower than expected and ENI attachment times have not yet returned to normal levels within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. ENI network state continues to propagate through the affected Availability Zone, but is expected to take another 15 - 30 minutes before we reach normal ENI attachment times. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;ENI network state continues to propagate through the affected Availability Zone (USE1-AZ4) further reducing ENI attachment times. Several affected services - including ELB, Glue and RDS - are now seeing full recovery, while other services - including EMR, EKS, and ECS - are experiencing limited impact at this stage. While we have not yet seen ENI attachment times return to normal levels just yet, we continue to make progress in resolving the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. Well continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">Apr 1,  1:45 AM PDT</span>&nbsp;Starting at 5:03 PM PDT, we experienced increased ENI attachment times for newly launched EC2 instances and newly attached ENIs within a single Availability Zone  (USE1-AZ4) in the US-EAST-1 Region. The issue was caused by increased resource contention in the subsystem responsible for the propagation of ENI network mappings within the affected Availability Zone. Engineers worked to identify the root cause of the resource contention and took steps to resolve it. By 7:45 PM PDT, ENI attachment times had returned to low single digit minute levels, which allowed most workflows to proceed and limited the impact to other AWS services. Some internal services, such as EMR, weighted away from the affected Availability Zone, which mitigated the impact. ENI attachment times continued to improve as the resource contention issue was addressed, and by 12:50 AM PDT, ENI attachment times had returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Power event impacting some instances",
      "date": "1649238391",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:46 AM PDT</span>&nbsp;Starting at 1:23 AM PDT, some EC2 instances experienced a loss of power and some EBS volumes experienced degraded performance within a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. Power was quickly restored to the affected instances and EBS volumes and by 1:40 AM PDT, the majority of EC2 instances and EBS volumes had fully recovered. By 2:05 AM PDT, the vast majority of affected EC2 instances and EBS volumes had fully recovered. Some RDS databases were also affected by the event, and recovered after power was restored. Customers with affected EC2 instances and EBS volumes were notified via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (N. Virginia)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1650382771",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption by Default in the US-EAST-1 Region for three Availability Zones: use1-az2, use1-az6 and use1-az5.  Existing instances are not affected, and no other launch requests are affected.  We have identified the root cause and are working towards recovery. </div><div><span class=\"yellowfg\"> 9:18 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that use EBS Encryption by Default in the US-EAST-1 Region.  Existing instances are not affected, and no other launch requests are affected. We have identified the root cause.  We are seeing recovery in use1-az5.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:46 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT. We began seeing significant recovery in use-az6 at 10:20 AM PDT and are beginning to see recovery in use1-az2.</div><div><span class=\"yellowfg\">11:30 AM PDT</span>&nbsp;Between 6:35 AM and 11:00 AM PDT we experienced increased error rates for new instance launches in the US-EAST-1 Region. Existing instances were unaffected by this issue. By 9:05 AM, we had full recovery in use1-az5. By 10:20 AM, we had full recovery in use1-az6, and at 11:00 AM we had recovered the final Availability Zone, use1-az2. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Singapore)",
      "summary": "[RESOLVED] Increased Launch Error Rates",
      "date": "1650969049",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:30 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption from an unencrypted AMI in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. For a small number of volumes in the Availability Zone, increased EBS I/O latencies may also be experienced. Existing instances are not affected, and no other launch requests are affected. We have identified the root cause and are working towards recovery.</div><div><span class=\"yellowfg\"> 4:30 AM PDT</span>&nbsp;Between 1:32 AM and 3:46 AM PDT, we experienced increased error rates for new launches that used EBS volumes in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. A small number of volumes in the Availability Zone may have experienced increased EBS I/O latencies during this time, and this issue has also been resolved for the vast majority of customers. We will be directly notifying a small subset of customers who may still be affected by increased EBS I/O latencies via the AWS Health Dashboard.</div>",
      "service": "ec2-ap-southeast-1"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1651071794",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We are investigating increased Amazon Redshift API and Console error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:58 AM PDT</span>&nbsp;We continue to investigate increased Amazon Redshift API and Console error rates in the US-EAST-1 Region that impact cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers using temporary credentials.</div><div><span class=\"yellowfg\"> 9:34 AM PDT</span>&nbsp;We are narrowing in on the root cause of the issue causing increased Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue is impacting cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;Between 7:12 AM and 9:52 AM PDT, we experienced increased error rates in Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue impacted cluster management operations and certain customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Seoul)",
      "summary": "[RESOLVED] Network Connectivity Issues",
      "date": "1651363256",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:00 PM PDT</span>&nbsp;AP-NORTHEAST-2     (APNE2-AZ2)   EC2 , RDS      EBS     .     .AWS           AWS  (https://console.aws.amazon.com/support)  AWS   . | We are investigating connectivity issues for some EC2 instances, RDS databases and degraded performance for some EBS volumes in a single Availability Zone (APNE2-AZ2) in the AP-NORTHEAST-2 Region. We are working to resolve the issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;    4 29 AP-NORTHEAST-2        EC2   EBS       .     4 36 ,   EC2   EBS   .    EC2   EBS   , AWS          . RDS       . AZ          AZ        .       .AWS          AWS  (https://console.aws.amazon.com/support )  AWS   . | Starting at 4:29 PM PDT, we experienced a loss of power to the underlying hardware for some EC2 instances and EBS volumes within a single Availability Zone in the AP-NORTHEAST-2 Region. Power was restored at 4:36 PM PDT, and the affected EC2 instances and EBS volumes began to recover. At this stage, the majority of affected EC2 instances and EBS volumes have recovered, and we continue to work on the small number that are still affected. Some RDS databases also experienced connectivity issues during this time period. Multi-AZ databases successfully failed away from the affected Availability Zone, but Single-AZ databases would remain impaired until the power was restored. Other Availability Zones are not affected by this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:31 PM PDT</span>&nbsp;   EC2   EBS      . EC2  EBS   ,      ( i3 ) ,      EBS   .RDS Aurora   AZ RDS           .  AZ RDS        .  AZ           ,        .API Gateway      M-TLS        .AWS     EC2   EBS     .AWS          AWS  (https://console.aws.amazon.com/support)  AWS   . | We can confirm that power has been restored to the underlying hardware of all affected EC2 instances and EBS volumes. EC2 instances and EBS volumes continue to recover and we now have a small number of instances  mostly i3 instances  and degraded performance for a small number of EBS volumes in the affected Availability Zone. Single-AZ RDS databases  including Amazon Aurora  experienced impact in the affected Availability Zone, but are recovering as power is restored. All Multi-AZ RDS databases have failed away from the affected Availability Zone. A small number of Single-AZ Application Load Balancers experienced elevated packet loss within the affected Availability Zone, but other load balancing traffic was shifted to other Availability Zones. API Gateway experienced elevated packet loss for M-TLS requests within the affected Availability Zone, but has now fully recovered. We continue to work on the small number of EC2 instances and EBS volumes that are still affected. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 6:00 PM PDT</span>&nbsp;  EC2   EBS   .      EBS  ,      . EC2   EBS           .  EC2   EBS     . EC2 , EBS   RDS                .AWS          AWS  (https://console.aws.amazon.com/support)  AWS   . | We have recovered the vast majority of the affected EC2 instances and EBS volumes. We have a small number of EBS volumes that are still experiencing degraded performance and continue to work to resolve them. Some EC2 instances and EBS volumes may have been hosted on hardware that was not recoverable after the loss of power. Customers will receive retirement notices for EC2 instances and EBS volumes where that is the case. If you continue to see impact for an EC2 instance, EBS volume, or RDS database, we recommend taking steps to relaunch or recreate the affected resource. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div>",
      "service": "ec2-ap-northeast-2"
    },
    {
      "service_name": "AWS IoT Core (Oregon)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1651634279",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for Connect, Subscribe &amp; Publish operations in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:40 PM PDT</span>&nbsp;We have identified the root cause for increased error rates and latency of Connect, Subscribe and Publish operations for new connections in the US-WEST-2 Region and are working towards resolution. A scheduled update to optimize the underlying infrastructure for the AWS IoT registry resulted in lower memory availability than required - this has been corrected, and recovery is in progress. Existing connections are not affected.</div><div><span class=\"yellowfg\"> 8:55 PM PDT</span>&nbsp;Between 7:17 PM and 8:41 PM PDT, we experienced increased error rates and latency for Connect, Subscribe, Publish and Registry operations in the US-WEST-2 Region. Existing connections were not affected by the event. The issue has been resolved and the service is operating normally.</div>",
      "service": "awsiot-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Ireland)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1652721211",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Some other AWS services are also affected by this issue, which we will provide further details on shortly. We are working to determine root cause and resolve the issues.</div><div><span class=\"yellowfg\">10:31 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Both the error rates and latencies are at a low enough level where retries would allow for a request to succeed. We have identified the root cause and are working to fully resolve the issue. The following AWS services are experiencing low levels of error rates as a result of this issue: Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Between 9:39 AM and 10:27 AM PDT, we experienced increased error rates and latencies for API Gateway in the EU-WEST-1 Region. All AWS services - including Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker - have now recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-eu-west-1"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors and latencies",
      "date": "1652830549",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:35 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.</div><div><span class=\"yellowfg\"> 5:09 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact to your account events. Well continue to update this post as we have more information to share.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for ELB APIs in the US-EAST-1 Region and have taken steps to mitigate the issue. We are now seeing recovery for the affected ELB APIs. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. We will continue to monitor until we can confirm full recovery.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 3:58 PM and 5:28 PM PDT, we experienced increased error rates and latencies for some ELB APIs in the US-EAST-1 Region. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "Amazon Kinesis Data Streams (Stockholm)",
      "summary": "[RESOLVED] Elevated API latencies and error rates and packetloss",
      "date": "1652985887",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We are investigating elevated API errors and latencies and elevated packet loss in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\">12:08 PM PDT</span>&nbsp;Between 11:12 AM and 11:45 AM PDT, we experienced elevated API errors and latencies and connectivity issues in the EU-NORTH-1 Region. The issue has been resolved and the services are operating normally.</div>",
      "service": "kinesis-eu-north-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Increased Errors Managing Payment Methods",
      "date": "1653519230",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:53 PM PDT</span>&nbsp;Between 12:30 PM and 2:10 PM PDT we experienced increased errors managing payment methods. The issue is resolved and the service is operating normally.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Intermittent DNS resolution failures",
      "date": "1653618095",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:21 PM PDT</span>&nbsp;We are aware of intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside AWS.  DNS queries for domains hosted on Route 53 are operating without any issues at this time.  We are actively working with the third-party DNS provider to resolve the issue as quickly as possible.</div><div><span class=\"yellowfg\"> 8:07 PM PDT</span>&nbsp;We confirm intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside of AWS. The third-party DNS provider is working toward resolution. We are also working toward a resolution that addresses the issues the third-party provider has encountered. Queries for DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 8:42 PM PDT</span>&nbsp;We can confirm the start of recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider.  We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:01 PM PDT</span>&nbsp;We can confirm the broad recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider. We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\">10:24 PM PDT</span>&nbsp;Between 5:35 PM and 9:58 PM PDT, we experienced intermittent DNS resolution issues for certain specific AWS endpoints. We can confirm that DNS resolution issues for these AWS names have been resolved. Since DNS answers for some of these names that were affected by issues with the third-party DNS provider could have been cached on AWS DNS resolvers, we are flushing these resolver caches over the next few hours. If you run your own DNS Resolvers, and you experience DNS resolution issues we suggest to flush your DNS Resolver cache. Queries for the DNS records hosted on Route 53 were not affected by this issue.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Load Balancing (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1654825968",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:52 PM PDT</span>&nbsp;We are investigating an increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. Existing load balancers are not affected by the issue. We are working to identify the root cause and resolve the issue.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. This is causing increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We are working to resolve the issue and expect to see an improvement in error rates as that progresses.</div><div><span class=\"yellowfg\"> 7:54 PM PDT</span>&nbsp;We continue to make progress towards resolving the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. While error rates have stabilized, we continue to see increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We will continue to keep you updated on our progress.</div><div><span class=\"yellowfg\"> 8:13 PM PDT</span>&nbsp;Starting at 7:46 PM PDT we experienced higher error rates and latencies for AWS services within the US-EAST-1 region. These error rates and latencies have seen some improvement from 7:55 PM PDT, but remain elevated. The issue is also affecting the AWS Management Console for the US-EAST-1 region. We continue to work towards mitigating the impact and will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 8:38 PM PDT</span>&nbsp;We continue to work on addressing the error rates and latencies for AWS services in the US-EAST-1 Region. The issue initially affected the Elastic Load Balancing APIs, with some impact to other services, including Elastic Container Service, Amazon Certificate Manager, and Directory Services. At 7:46 PM PDT, other AWS services began to experience an increase in error rates and latencies. Action was taken and error rates started to improve at 7:55 PM PDT. This issue also affected access to the the AWS Management Console for the US-EAST-1 Region. Error rates and latencies for services in the US-EAST-1 Region remain elevated for a number of services, including Connect, DynamoDB, SQS, SNS, EC2, CloudFormation, CloudFront, amongst others. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\"> 9:13 PM PDT</span>&nbsp;We continue to see a reduction in error rates and latencies for AWS services within the US-EAST-1 Region as we work to resolve the issue. While many AWS services are experiencing elevated error rates and latencies, services that are experiencing higher error rates have been tagged on this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. The AWS Management Console is operating normally, however some customers may observe API Errors at times. AWS Connect error rates have improved as well and we continue to work towards full recovery. We are working on applying mitigations to fully resolve the issue.</div><div><span class=\"yellowfg\"> 9:32 PM PDT</span>&nbsp;We continue to see a reduction in API error rates and latencies for services within the US-EAST-1 Region. Elastic Load Balancer APIs have recovered and returned to normal levels. The AWS Management Console has recovered. AWS Connect error rates have returned to normal levels. CloudWatch metrics have recovered and the EC2 API error rates have returned to normal levels. DynamoDB has recovered and is operating normally. Services that were affected by this event remain tagged to this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. </div><div><span class=\"yellowfg\"> 9:58 PM PDT</span>&nbsp;Starting at 6:01 PM PDT, we experienced elevated error rates and latencies for AWS services within the US-EAST-1 Region. The issue affected AWS service APIs, with no impact to data plane services such as EC2 instances, EBS volumes, or Elastic Load Balancers. We started to see recovery at 7:55 PM PDT and were fully recovered by 9:25 PM PDT. The issue has been resolved and the service is operating normally.</div>",
      "service": "elb-us-east-1"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Incorrect Free Tier Email Alerts",
      "date": "1655342154",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:15 PM PDT</span>&nbsp;We are investigating customers receiving email alerts about Free Tier Usage which does not match what customers are seeing in the billing console.</div><div><span class=\"yellowfg\"> 6:39 PM PDT</span>&nbsp;Between 11:00 AM and 6:00 PM PDT, some customers received incorrect email alerts about their Free Tier Usage. This was caused by an incorrect software deployment, which has since been rolled back. During this time, the AWS Billing Console was reporting the correct information. Customers who received an incorrect email alert will receive a notification via the AWS Health Dashboard Event Log in your AWS account.</div><div><span class=\"yellowfg\"> 6:53 PM PDT</span>&nbsp;We want to clarify our previous post. Between 11:00 AM and 6:00 PM PDT, the Home page and the Bills page on the AWS Billing Console reported correct information. The Free Tier page on the AWS Billing Console displayed the same incorrect information included in the incorrect email alert.  We have corrected the issue that caused both the incorrect emails and the incorrect information in the Free Tier page on the AWS Billing Console, and they are all reporting correct information now.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "AWS Billing Console",
      "summary": "[RESOLVED] Availability Decreased",
      "date": "1657032030",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 7:40 AM PDT</span>&nbsp;We can confirm a decrease in the Cost Explorer Console availability. We are actively working towards resolution.</div><div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We have identified the cause of the issue affecting Cost Explorer and are working towards resolution. Customers using the Cost Explorer console may experience long load times, timeouts and errors using the console. Cost Explorer APIs are experiencing elevated latencies and error rates. The issue is limited to the Cost Explorer console and APIs, AWS Billing is not otherwise impacted.</div><div><span class=\"yellowfg\"> 9:19 AM PDT</span>&nbsp;We continue to investigate the issue affecting Cost Explorer and are working towards resolution.  We will provide the next update by 10:00 AM PDT.</div><div><span class=\"yellowfg\">10:01 AM PDT</span>&nbsp;We continue to investigate the issue affecting Cost Explorer and are working towards resolution.  We will provide the next update by 10:30am PDT.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;Between 05:10 AM and 10:27 AM PDT, customers using the Cost Explorer console experienced long load times, timeouts, or errors using the console. Cost Explorer APIs also experienced elevated latencies and error rates. The issue has been resolved and the service is operating normally. AWS Billing was otherwise not impacted.</div>",
      "service": "billingconsole"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (London)",
      "summary": "[RESOLVED] Impaired EC2 Instances",
      "date": "1657475798",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:56 AM PDT</span>&nbsp;We are investigating instance impairments in a single Availability Zone (EUW2-AZ1) in the EU-WEST-2 Region. Other Availability Zones are not affected by the event and we are working to resolve the issue.</div><div><span class=\"yellowfg\">11:13 AM PDT</span>&nbsp;Some instances in a single Availability Zone are currently impaired or have lost power in the EU-WEST-2 Region. The root cause is a thermal event within a data center in the affected Availability Zone that we are working to resolve. Some instances may also be experiencing network connectivity issues in the affected Availability Zone. Elastic Load Balancing has shifted traffic away from the Availability Zone. All multi-AZ databases have failed away from the affected Availability Zone, however single-AZ databases will remain affected until we see full recovery. We do not yet have a an ETA for full recovery but expect it to be more than an hour. We will provide further guidance as soon as we have it. For customers that are able to fail away from the affected Availability Zone, we recommend doing so.</div><div><span class=\"yellowfg\">11:55 AM PDT</span>&nbsp;We continue to investigate instance impairments to a single Availability Zone in the EU-WEST-2 Region. We have experienced an increase in temperatures within a single data center in the affected Availability Zone, which in some cases has caused impairments for instances in the Availability Zone. We have engineers within the affected data center and are working to resolve the issue. ELB has shifted traffic away from the affected Availability Zone, and API Gateway has mitigated the majority of impact and continues to work on the remaining endpoints. Elastic File System (EFS) is experiencing errors within the affected Availability Zone. RedShift, OpenSearch, and ElastiCache are experiencing error rates for clusters within the affected Availability Zone. RDS has successfully mitigated impact for all multi-AZ databases, however single-AZ databases will remain affected until we see recovery. Lambda is largely unaffected by the event, however a very small number of functions may be experiencing invocation errors within the affected Availability Zone. The EC2 APIs are experiencing increased error rates within the affected Availability Zone, but instance launches continue to work on other Availability Zones. Some services, like EMR and Fargate, are seeing delays in provisioning new instances in the affected Availability Zone due to the EC2 API impact in that Availability Zone. Our engineering teams continue to work towards identifying the root cause of the thermal event and resolving it. At this stage, we do not have an ETA but still expect it to be more than an hour. We continue to recommend using other Availability Zones in the EU-WEST-2 Region, which remain unaffected by this event.</div><div><span class=\"yellowfg\">12:25 PM PDT</span>&nbsp;We have resolved the root cause of the thermal event and are starting to see recovery for impaired EC2 instances within the EU-WEST-2 Region. At this stage, the vast majority of EC2 instances have recovered and we continue to work on the instances that are still affected. ELB and API Gateway have shifted traffic away from the affected Availability Zone. EFS is only experiencing error rates for one zone filesystems within the affected Availability Zone; standard filesystems, which use multiple Availability Zones, are not affected. Customers should now be seeing recovery for instance impairments, and we expect to see recovery for the vast majority of instances within the next hour. We will continue to provide updates as recovery continues.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;We continue to make progress in resolving the EC2 impaired instances in a single Availability Zone in the EU-WEST-2 Region. At this stage, the vast majority of affected instances have recovered and we continue to work towards full recovery. ELB and API Gateway remain weighted away from the affected Availability Zone for now. EFS has recovered the availability of all One Zone file systems in the affected Availability Zone. RDS multi-AZ databases remain available and we are starting to see recovery for single-AZ databases within the affected Availability Zone. EKS pods within the affected Availability Zone are starting to see recovery, however EKS did experience failures during cluster creation during the event. RedShift, OpenSearch, and ElastiCache are starting to see recovery within the affected Availability Zone. EC2 APIs have fully recovered and new instances can once again be launched in the affected Availability Zone. Customers should be seeing most of their affected instances in recovery, although we continue to work on a small number of affected EC2 instances and EBS volumes that are still impaired. We will continue to provide updates as recovery progresses.</div><div><span class=\"yellowfg\"> 1:53 PM PDT</span>&nbsp;We have resolved the impairments for the vast majority of EC2 instances in the affected Availability Zone (euw2-az1) in the EU-WEST-2 Region. There are a small number of EC2 instances and EBS volumes that were hosted on hardware that have been affected by the loss of power during the thermal event. For these EC2 instances and EBS volumes, we will be opening Personal Health Dashboard notices to track recovery. We have seen full recovery for a number of AWS services, including AWS Transit Gateway, Amazon Connect, Amazon Relational Database Service, Amazon ElastiCache, Amazon Elastic Container Service, Amazon Elastic File System, Amazon, Elastic Kubernetes Service, Amazon Elastic MapReduce, Amazon OpenSearch Service, and Amazon Redshift. The remaining AWS services, including Amazon API Gateway, Amazon CloudFront, Amazon Elastic Load Balancing are very close to full recovery at this stage. Elastic Load Balancing and API Gateway will be shifting traffic back into the affected Availability Zone shortly. Were working through the remaining EC2 instances and EBS volumes and expect to see full recovery in the next 30 minutes.</div><div><span class=\"yellowfg\"> 2:07 PM PDT</span>&nbsp;Starting at 10:25 AM PDT, some EC2 instances and EBS volumes were impaired in a single Availability Zone (euw2-az1) in the EU-WEST-2 Region. The issue was caused by a thermal event, which caused some of instances to lose power and some EBS volumes to experience degraded IO performance. Recovery started at 12:01 PM PDT and by 12:12 PM PDT, the vast majority of EC2 instances and EBS volumes had fully recovered. Other AWS services also experienced impact during the event, including API Gateway, Elastic Load Balancing, Database Migration Service, Transit Gateway, CloudFront, Connect, ElastiCache, Elastic Container Service, Elastic File System, Elastic Kubernetes Service, Elastic MapReduce, OpenSearch, Redshift, and Relational Database Service. These services are all now operating normally. There are a small number of EC2 instances and EBS volumes that were hosted on hardware that has been affected by the loss of power during the thermal event. For these EC2 instances and EBS volumes, we will be opening Personal Health Dashboard notices to track recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-eu-west-2"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased AWS SSO Management Console Error Rates",
      "date": "1658156924",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:08 AM PDT</span>&nbsp;We are investigating increased error rates in the AWS SSO management console in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:49 AM PDT</span>&nbsp;We can confirm an issue with the AWS Single Sign-On (SSO) service, which is affecting customers who are using an external identity provider that is enabled in the US-EAST-1 Region.  Affected customers experiencing issues when attempting SSO authentication for the AWS console and services.  We are actively working towards identifying the root cause.</div><div><span class=\"yellowfg\"> 9:10 AM PDT</span>&nbsp;We are seeing recovery of this issue since 8:41 AM PDT, and all AWS console and API requests authenticated with an external SSO provider in the US-EAST-1 Region should be working properly now. We are continuing to monitor systems before confirming full recovery.</div><div><span class=\"yellowfg\"> 9:43 AM PDT</span>&nbsp;Between 7:34 AM and 8:41 AM PDT AWS Single Sign-On (SSO) customers using an external identity provider that is enabled in the US-EAST-1 Region experienced issues when attempting SSO authentication for the AWS console and services.  The issue has been identified and mitigated, and all SSO authentication requests are working properly now.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Single Sign-On (N. Virginia)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1658178914",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:15 PM PDT</span>&nbsp;We are investigating increased error rates for SSO in the US-EAST-1 Region. </div><div><span class=\"yellowfg\"> 2:44 PM PDT</span>&nbsp;We can confirm an issue that is preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available. Existing sessions are not affected by this issue, but will experience difficultly re-authenticating should the existing session expire. This issue is affecting AWS services that use sign-in authentication, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. This issue does not affect authentication using IAM users or roles. We are working to resolve the issue and will provide you with regular updates.</div><div><span class=\"yellowfg\"> 3:17 PM PDT</span>&nbsp;We continue to work on resolving the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. We have identified the root cause of the issue, which is within the subsystem responsible for hosting the authentication page. Customers attempting to establish a new session or re-authenticate an expired session may experience increased error rates (503/504) when attempting to access the authentication page. In some cases, retries will succeed and allow for the session to be re-authenticated. Engineers are working to resolve the issue, but we do not yet have an ETA for resolution. We are working on mitigations now, and will provide a crisper ETA in the next update. The issue continues to affect AWS services using sign-in authentication, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. Note that should these services be configured to use other authentication types, such as IAM or SAML, they will not be affected by this issue. This issue does not affect authentication using IAM users or roles. We are working to resolve the issue and will provide you with regular updates.</div><div><span class=\"yellowfg\"> 3:50 PM PDT</span>&nbsp;We continue to work on resolving the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. Over the last 30 minutes we have made progress in addressing the issue and error rates with the authentication page have begun to improve. In many cases, retries are now working, allowing for authentication to be completed. While we have seen an improvement in error rates, they have not yet returned to normal levels, so engineers remain engaged and working to fully resolve the issue. We expect error rates to continue to drop over the next hour, but are still not certain on an exact ETA for full resolution of the event.</div><div><span class=\"yellowfg\"> 4:37 PM PDT</span>&nbsp;We have seen error rates improve substantially for the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page is not available in the US-EAST-1 Region. In several cases, customers are seeing recovery and have been able to authenticate their sign-in requests. We continue to work on the remaining error rates, which continue to cause some issues with the access to the authentication page. At this stage, retrying requests should allow for authentication requests to succeed. This issue does not affect authentication using IAM users or roles. We will continue to provide updates as we work to fully resolve the issue.</div><div><span class=\"yellowfg\"> 4:58 PM PDT</span>&nbsp;We have resolved the issue preventing customers from establishing new sessions or re-authenticating existing sessions as the authentication page was not available in the US-EAST-1 Region. Beginning at 12:35 PM PDT, customers experienced errors when attempting to authenticate using the sign-in page in the US-EAST-1 Region. Engineers worked to resolve the issue, and at 3:23 PM PDT error rates began to improve, allowing customers to authenticate their sign-in requests. At 3:41 PM PDT, error rates had returned to normal levels and the authentication page was once again available and serving sign-in requests from affected AWS services, including Single Sign-on, Workspaces, WorkMail, Connect, Directory Service, QuickSight and WorkDocs. Between 4:12 PM and 4:38 PM PDT, we experienced an increase in error rates once again, but this has also been resolved. This issue did not affect authentication using IAM users or roles, or SAML. The issue has been resolved and all services are operating normally.</div>",
      "service": "sso-us-east-1"
    },
    {
      "service_name": "AWS Secrets Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Errors and Latencies",
      "date": "1658948667",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:04 PM PDT</span>&nbsp;We are investigating increased API connection timeout errors and latencies in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:33 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for Secrets Manager APIs in the US-EAST-1 Region. We are working to identify root cause and resolve the issue. \n\nCustomers using Secrets Manager to store secrets may experience errors when using the Secrets Manager APIs, including customers invoking Secrets Manager from Lambda functions. \n\nLambda functions that are not calling Secrets Manager APIs are not affected by this event. We continue to work to understand the issue and will continue to provide updates. </div><div><span class=\"yellowfg\">12:57 PM PDT</span>&nbsp;We have resolved the issue causing increased error rates and latencies for the Secrets Manager APIs in the US-EAST-1 Region. Customers using Secrets Manager to store secrets may have experienced errors when using the Secrets Manager APIs, including customers invoking Secrets Manager from Lambda functions. Lambda functions that are not using Secrets Manager were not affected by this event. The issue was resolved at 12:30 PM PDT, when error rates and latencies returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "secretsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Instance Impairments",
      "date": "1659028265",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:11 AM PDT</span>&nbsp;We are investigating network connectivity issues for some instances and increased error rates and latencies for the EC2 APIs within the US-EAST-2 Region.</div><div><span class=\"yellowfg\">10:25 AM PDT</span>&nbsp;We can confirm that some instances within a single Availability Zone (USE2-AZ1) in the US-EAST-2 Region have experienced a loss of power. The loss of power is affecting part of a single data center within the affected Availability Zone. Power has been restored to the affected facility and at this stage the majority of the affected EC2 instances have recovered. We expect to recover the vast majority of EC2 instances within the next hour. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">10:49 AM PDT</span>&nbsp;We continue to see recovery of EC2 instances that were affected by the loss of power in a single Availability Zone in the US-EAST-2 Region. At this stage, the vast majority of affected EC2 instances and EBS volumes have returned to a healthy state and we continue to work on the remaining EC2 instances and EBS volumes. Elastic Load Balancing has shifted traffic away from the affected Availability Zone. Single-AZ RDS databases were also affected and will recover as the underlying EC2 instance recovers. Multi-AZ RDS databases would have mitigated impact by failing away from the affected Availability Zone. While the vast majority of Lambda functions continue operating normally, some functions are experiencing invocation failures and latencies, but we expect this to improve over the next 30 minutes. Power has been restored to all affected resources and remains stable. We expect the recovery of EC2 instances and EBS volumes to continue to improve over the next 45 minutes. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">11:25 AM PDT</span>&nbsp;We continue to make progress in recovering the remaining EC2 instances and EBS volumes affected by the loss of power in a single Availability Zone in the US-EAST-2 Region. The vast majority of EC2 instances are now healthy, but we continue to work on recovering the remaining EBS volumes affected by the issue. EC2 API error rates and latencies have returned to normal levels. Elastic Load Balancing remains weighted away from the affected Availability Zone. Error rates and latencies for Lambda function invocations have now returned to normal levels. Power has been restored to all affected resources and remains stable. We expect the recovery of EC2 instances and EBS volumes to continue to improve over the next 30 minutes. For customers that need immediate recovery, we recommend failing away from the affected Availability Zone as other Availability Zones are not affected by this issue.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;At this stage, the vast majority of EC2 instances and EBS volumes, affected by the loss of power in a single Availability Zone in the US-EAST-2 Region, have fully recovered. A small number of EC2 instances and EBS volumes are on hardware that was adversely affected by the loss of power. Engineers continue to work on recovering the EC2 instances and EBS volumes on this hardware and will provide updates via the Personal Health Dashboard if any of these could not be recovered. Elastic Load Balancers affected by the issue have recovered and traffic has been shifted back into the affected Availability Zone. The vast majority of single-AZ databases have also recovered and the remaining databases are running on hardware that was affected by the event. We will provide updates via the Personal Health Dashboard if any of these databases can not be recovered. At this stage, if your EC2 instance or EBS volumes that has still not recovered, attempting a reboot of the EC2 instance could resolve the issue. If not, we recommend relaunching the affected EC2 instance or recreating the affected EBS volume.</div><div><span class=\"yellowfg\">12:45 PM PDT</span>&nbsp;Starting at 9:57 AM PDT some EC2 instances and EBS volumes experienced a loss of power within a single Availability Zone in the US-EAST-2 Region. Power was restored at 10:19 AM PDT and EC2 instances and EBS volumes began to recover. By 10:23 AM PDT, the vast majority of EC2 instances and EBS volumes has fully recovered and by 11:37 AM PDT, all but a very small number of EC2 instances and EBS volumes had recovered. Elastic Load Balancing shifted traffic away from the affected Availability Zone, which has now been shifted back. RDS impact for single-AZ databases, which have also been recovered. Other services (tagged below) saw impact during the event, but most have fully recovered. Those that are still seeing impact, will provide updates via the Personal Health Dashboard as they work towards full recovery. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS CloudTrail (N. Virginia)",
      "summary": "[RESOLVED] CloudTrail event delivery delays",
      "date": "1660680453",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:07 PM PDT</span>&nbsp;We are investigating increased latency of CloudTrail events causing CloudTrail events processing delays in the US-EAST-1 Region starting at 9:33 AM PDT. CloudTrail customers in US-EAST-1 Region will receive events with delay as high as 4 hours.  All new events after 12:57 PM PDT will be processed immediately. ETA for backlog consumption is 3:30 PM PDT.</div><div><span class=\"yellowfg\"> 2:10 PM PDT</span>&nbsp;Between 09:33 AM and 12:57 PM PDT Increased latency of CloudTrail events in the US-EAST-1 Region causing CloudTrail events processing delays. We have resolved the issue all backlog events are in the process of backfilling will be completed by 3:30 PM PDT. The service is operating normally. </div>",
      "service": "cloudtrail-us-east-1"
    },
    {
      "service_name": "Multiple services (Oregon)",
      "summary": "[RESOLVED] Increased API Errors",
      "date": "1661361799",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies affecting some services within the US-WEST-2 Region. We are working to identify the root cause.</div><div><span class=\"yellowfg\">10:34 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies affecting some services within the US-WEST-2 Region. The root cause appears to be increased error rates and latencies for API Gateway endpoints within the US-WEST-2 Region. Customers with API Gateway endpoints would be experiencing increased error rates and latencies for their requests. Customers calling Lambda via an API Gateway, would also be experiencing increased error rates and latencies for their function invocations. Other AWS services that use API Gateway - for example Batch, EKS, and EventBridge - are also experiencing increased error rates and latencies. Amazon Connect is experiencing increased call failure as well as issues with user login. We have identified the root cause and are working on a mitigation to resolve the issue.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;We have addressed the root cause of the issue causing increased API error rates and latencies in the US-WEST-2 Region and are seeing recovery across affected API Gateway endpoints and AWS services.</div><div><span class=\"yellowfg\">10:53 AM PDT</span>&nbsp;Between 10:00 AM and 10:34 AM PDT we experienced increased API error rates and latencies affecting some services within the US-WEST-2 Region. The root cause of the event was increased error rates and latencies for API Gateway endpoints within the US-WEST-2 Region. Customers calling Lambda functions via an API Gateway, also experienced increased error rates and latencies for their function invocations. Other AWS services that use API Gateway - for example Batch, EKS, and EventBridge - also experienced increased error rates and latencies. Amazon Connect saw increased call failures as well as issues with user logins. We have resolved the root cause of the event and all affected API Gateway endpoints, Lambda functions, and AWS services have fully recovered. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-us-west-2"
    },
    {
      "service_name": "Amazon Elastic Container Service (N. Virginia)",
      "summary": "[RESOLVED] Amazon Elastic Container Service - Increased Rates of Insufficient Capacity Errors",
      "date": "1661380251",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:30 PM PDT</span>&nbsp;We can confirm increased insufficient capacity error rates for launching new ECS tasks using the Fargate launch type, starting at 1:15 PM PDT. Running tasks and tasks that utilize the EC2 launch type are not impacted. We continue to work through full resolution and will provide another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:06 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work through full resolution of the issue. We expect you to begin observing signs of recovery beginning at 4:20 PM PDT. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue. This is taking longer than expected, and we now expect it to be 5:00 PM PDT before you will begin to observe signs of recovery. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:20 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue, however we are experiencing some delays with full recovery. You will still see some task launches succeeding during this event. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:49 PM PDT</span>&nbsp;We have identified the root cause for the increase in insufficient capacity error rates for launching new Fargate tasks and pods. Customers using ECS with Fargate and EKS with Fargate are impacted, starting at 1:15 PM PDT. We continue to work towards full resolution of the issue, however we are experiencing some delays with full recovery. We are working multiple, parallel paths to make additional capacity available. You will still see some task launches succeeding during this event. Running tasks and pods are not impacted. Customers using ECS with EC2 or EKS with EC2 are not impacted by this issue. We will provide an another update in the next 30 minutes.</div><div><span class=\"yellowfg\"> 6:49 PM PDT</span>&nbsp;We have identified the cause of the decreased capacity and understand why the Fargate task launch success rate is only 70% at this point. We are working on multiple parallel actions to address the underlying issues and have identified one area in particular that should help us make faster progress towards recovery. We have started work on this and will have an indication of progress by 7:00 PM PDT. Once we have that progress data we will be able to provide an ETA for recovery. We are also making a change to the rate at which ECS launches tasks to reduce load on Fargate and to speed up recovery.\nFor customers with prepared and rehearsed plans for moving to a different region, they should consider exercising those if they are in a place to do so. Customers can also consider using EC2 with ECS and EKS as a mitigation option since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 7:45 PM PDT</span>&nbsp;We have identified the cause of the decreased capacity and understand why the Fargate task launch success rate is only 70% at this point. Our remediation actions are making slower progress than expected, so we are working on additional actions to further reduce load on Fargate. The work started in the previous update is still progressing but we do not yet have a projected ETA for when it will complete or when we will see recovery.\n\nCustomers can switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 8:46 PM PDT</span>&nbsp;The current state is that more than 50% of Fargate task launches in the US-EAST-1 region are succeeding. For the tasks that are failing, we have identified that this is due to a large amount of compute capacity, managed by Fargate, that is in a stuck state. We call these leaked instances. This results in customer tasks and pods not being able to be started, impacting both ECS on Fargate and EKS on Fargate. \n\nWe are driving multiple parallel efforts to address this. First, we're taking action to make sure no additional instances are leaked by reducing the call rates to Fargate, and second we are working to free up these leaked instances so they can be used to run customer tasks. \n\nAs stated in previous updates, the recovery actions are making slower progress than we expected which is preventing us from providing an ETA for recovery at this point. Right now we expect multiple hours before we see recovery. \n\nCustomers who already have Fargate tasks or pods running are recommended to not scale down until we have recovery. Customers can switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 9:19 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances, and we are already seeing faster progress towards this.\n\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will steadily ramp up Fargate task and pod launches and enable normal operations. We don't yet have an ETA, but will communicate one as soon as we have an ETA we feel confident about.\n\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\"> 9:55 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We estimate that we have released 50% of the leaked instances and at the current rate that all leaked instances will have been released by 12:00 AM PDT. \n\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will steadily ramp up Fargate task and pod launches and enable normal operations. We expect to be able to communicate an ETA for recovery soon after we complete releasing all leaked instances. Once this happens, our best estimate is an additional 1 to 2 hours for recovery. \n\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">10:26 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We estimate that we have released 93% of the leaked instances and at the current rate that all leaked instances will have been released by 10:45 PM PDT.\nOnce we have released all the leaked instances and are seeing recovery in our other metrics, we will slowly ramp up Fargate task and pod launches and enable normal operations. We expect to be able to communicate an ETA for recovery soon after we complete releasing all leaked instances. Once this happens, our best estimate is an additional 1 to 2 hours for recovery.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">11:42 PM PDT</span>&nbsp;To help speed up recovery, we have temporarily disabled Fargate task and pod launches in the US-EAST-1 region. While we are in this state, you will see all Fargate task and pod launches fail. Disabling Fargate task launches will free the service up to process and release the leaked instances. We have now released effectively all leaked instances and are now starting the process to re-enable Fargate task launches.\nWe will start by enabling Fargate task launches for a small number of accounts and then increase that as we see success. As the pace of recovery is dependent on how fast we increase task launches, it is still too early to provide an ETA for full recovery, but we still expect hours before we are fully recovered.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25, 12:46 AM PDT</span>&nbsp;We have started to enable Fargate task launches in the US-EAST-1 Region again. We are seeing successful task launches and continue to slowly increase the number of tasks being launched. Most customers will still see their task launches failing until we see enough progress to broadly enable task launches. We expect to have enough data to further increase task launches by 1:10 AM PDT.\nThe progress at that point will help inform ETA for recovery, we still estimate this to be hours out.\nIn the meantime, customers are recommended to avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  1:55 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. At this point, all accounts can launch tasks, but at a lower task launch rate than usual. The lower than normal task launch rates means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. The impact of this is that scaling of services and deployments will take longer than usual. We will incrementally raise task launch rates back to normal levels as we monitor service recovery.\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 4:00 AM PDT.\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  3:08 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. We have just completed a change that increases the task launch rate for tasks running as part of an ECS service. This will reduce the time required both for service deployments and scaling up services. As a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. We will continue to incrementally raise the task launch rate back to normal levels as we monitor service recovery.\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 5:00 AM PDT.\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:03 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. In addition to the earlier change to increase the task launch rate for tasks running as part of an ECS service, we have also increased the task launch rate for the RunTask API from 1 task per second to 2 tasks per second. As a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed. We will continue to incrementally raise the task launch rate back to normal levels as we monitor service recovery.\n\nWe are no longer seeing elevated task launch failures and our current estimate for full recovery is 5:00 AM PDT.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:34 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. The task launch rate for the ECS RunTask API has now been increased from 2 tasks per second to 5 tasks per second and we are seeing a corresponding reduction in task launch failures due to customers exceeding the maximum task launch rate. We are continuing to monitor recovery and will keep incrementally raising the task launch rate until we return to the default of 20 tasks per second.\n\nAs a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed.\n\nWe are no longer seeing elevated task launch failures. It is, however likely to be past 5:00 AM PDT before we have returned to the default task launch rate of 20 tasks per second.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  4:57 AM PDT</span>&nbsp;We continue making progress with enabling Fargate task launches in the US-EAST-1 Region. All customers now have a task launch rate for the ECS RunTask API of 10 tasks per second and we see further reduction in task launch failures due to customers exceeding the maximum task launch rate. We are continuing to monitor recovery and will keep incrementally raising the task launch rate until we return to the default of 20 tasks per second.\n\nAs a reminder, all customers are unblocked from launching Fargate tasks at this point, although still at a rate lower than usual. The lower than normal task launch rate means customers can still see task launches failing due to attempting to launch tasks at a higher rate than currently allowed.\n\nWe are no longer seeing elevated task launch failures. It is, however likely to be past 5:00 AM PDT before we have returned to the default task launch rate of 20 tasks per second.\n\nWe still recommend that customers avoid scaling down tasks and pods as they will not be able to launch new tasks until we re-enable Fargate task and pod launches. Customers can also switch to using EC2 with ECS and EKS as a mitigation since ECS with EC2 and EKS with EC2 are not impacted by this event.</div><div><span class=\"yellowfg\">Aug 25,  6:05 AM PDT</span>&nbsp;Between August 24 1:07 PM and August 25 6:00 AM PDT, ECS and EKS customers using Fargate experienced increased task and pod launch failure rates. The issue has been resolved and customers are now able to launch tasks and pods normally again. A very small number of customers may still see failing task and pod launches and we will provide updates to those customers using account specific events. ECS and EKS customers using EC2 were not impacted during this event and there was also no impact to any already running tasks or pods on Fargate.</div>",
      "service": "ecs-us-east-1"
    },
    {
      "service_name": "AWS Systems Manager (N. Virginia)",
      "summary": "[RESOLVED] Increased API Error Rates and Latency",
      "date": "1662737760",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:36 AM PDT</span>&nbsp;We are investigating increased API error rates and latency in the US-EAST-1 Region. This will impact customers' ability to make calls to Parameter Store.</div><div><span class=\"yellowfg\"> 9:29 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of data may be affected by the event as they are unable to retrieve the data. We have identified the subsystem where the errors are occurring and are working to resolve the issue. At this stage, we expect recovery to take more than an hour but will keep you updated on our progress.</div><div><span class=\"yellowfg\"> 9:50 AM PDT</span>&nbsp;We continue to work on the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configurations may be affected by the event as they are unable to retrieve the data. Customers invoking Lambda functions that use Environment Variables will also experience increased error rates and latencies. The subsystem responsible for the Parameter Store is experiencing resource contention, which is leading to the increased error rates and latencies. We continue to work toward the resolving the issue, but do not have an ETA on recovery at this stage. We will keep you updated on our progress.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;We continue to work on the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration may be affected by the event as they are unable to retrieve the data. We have confirmed that customers using Lambda Environment Variables are not affected by this event, but rather customers that are attempting to access the Parameter Store directly from a Lambda function. Customers can work around the issue by removing the use of the Parameter Store from their code, or look to use some other store, such as DynamoDB. We are making progress towards resolution but continue to see elevated error rates for the affected subsystem. We do not have an ETA on recovery at this stage and will keep you updated on our progress.</div><div><span class=\"yellowfg\">11:02 AM PDT</span>&nbsp;We are seeing recovery for the issue causing increased API error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration should be seeing recovery at this stage. We continue to monitor the subsystem that was experiencing resource contention, but expect error rates and latencies to remain at normal levels.</div><div><span class=\"yellowfg\">11:21 AM PDT</span>&nbsp;Starting at 7:34 AM PDT, we experienced increased error rates and latencies for the Systems Manager Parameter Store in the US-EAST-1 Region. Customers using the Parameter Store for the storage of configuration were affected by the event as they were unable to retrieve the data. Customers using Parameter Store from Lambda functions also experienced increased failure rates. The issue was resolved at 10:45 AM PDT, when error rates and latencies returned to normal levels. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2systemsmanager-us-east-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Cape Town)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1663153260",
      "status": "2",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:01 AM PDT</span>&nbsp;We are investigating increased API error rates in the AF-SOUTH-1 Region. Existing instances are unaffected.</div><div><span class=\"yellowfg\"> 4:21 AM PDT</span>&nbsp;We are experiencing elevated API errors and latency with the IAM authentication service in the AF-SOUTH-1 Region.  This is causing impact on services that require IAM authentication.  We are actively investigating root cause to identify a path to mitigation.</div><div><span class=\"yellowfg\"> 5:13 AM PDT</span>&nbsp;We continue to see elevated API and latency for IAM authentication and authorization requests in the AF-SOUTH-1 Region. The IAM errors affect any AWS API call that requires authentication, though some calls may still be succeeding due to API credential caching. We have all teams engaged in investigation of root cause and mitigation, but we do not have an ETA for recovery at this time.</div><div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;We are starting to see recovery of IAM authentication and authorization requests, and customers should be seeing improvements in API error rates in the AF-SOUTH-1 Region.  We are continuing to monitor and work towards full recovery.</div><div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are continuing to see improvement in IAM authentication and authorization API error rates, and a subsequent improvement in AWS service API availability in the AF-SOUTH-1 Region. We continue to monitor this event closely, and will do so until all services have fully recovered.</div><div><span class=\"yellowfg\"> 6:22 AM PDT</span>&nbsp;All service APIs are now operating correctly, and we continue to work towards full recovery of services in the AWS Management Console. Were continuing to restore normal operations to all AWS consoles in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 6:54 AM PDT</span>&nbsp;Between 3:30 AM and 6:35 AM PDT, customers experienced elevated errors for API and console requests in the AF-SOUTH-1 Region due to an issue with IAM authentication and authorization, which caused elevated error rates for many AWS services in the AF-SOUTH-1 Region.  This issue has been resolved and all services are now working normally.</div>",
      "service": "ec2-af-south-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased Route 53 Health Check API latency",
      "date": "1663585076",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:57 AM PDT</span>&nbsp;We are experiencing delays creating, deleting, and updating Route 53 health checks.  This is also affecting provisioning and scaling workflows for services like ELB, EKS, and RDS that use Route 53 health checks as well.  We have identified the issue and have a path towards recovery, however we do expect that to take at least a couple hours.\n\nExisting Route 53 health checks are operating normally, and are correctly reflecting the health status of their configured resources. \n</div><div><span class=\"yellowfg\"> 4:32 AM PDT</span>&nbsp;We continue to work towards recovery of the Route 53 Health Check API, and expect health check create, update, and delete delays to be resolved within about 90 minutes.  However, it will take a bit longer for other impacted services to fully recover their own provisioning and scaling workflows as they have their own backlogs to work through.</div><div><span class=\"yellowfg\"> 5:28 AM PDT</span>&nbsp;The Route 53 health check API has recovered, and all operations are now being processed normally.  Other services that were impacted by this issue are processing their scaling and provisioning backlogs.</div><div><span class=\"yellowfg\"> 6:25 AM PDT</span>&nbsp;Between 12:00 AM and 5:05 AM PDT, customers experienced elevated errors and latency when calling the Route 53 Health Checks API.  This issue also affected provisioning and scaling workflows for some other AWS services.  The Route 53 Health Checks API is now working normally.  ELB, EKS, EMR, and Amazon MQ are still processing backlogs, and affected customers can get more information in Account Specific Service Events.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Mumbai)",
      "summary": "[RESOLVED] Increased API error rates",
      "date": "1663816639",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased API error rates for DetachVolume, AttachVolume, and AssociateIamInstanceProfile in the AP-SOUTH-1 Region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 8:35 PM PDT</span>&nbsp;Between 6:26 PM and 8:23 PM PDT we experienced increased API error rates for DetachVolume, AttachVolume, and AssociateIamInstanceProfile in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-ap-south-1"
    },
    {
      "service_name": "AWS Systems Manager (Oregon)",
      "summary": "[RESOLVED] Increased API Error Rates",
      "date": "1664323892",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;We are experiencing increased latency and error rates when calling the StartSession API for a subset of customers in the US-WEST-2 Region. We have identified the issue and working towards resolution.</div><div><span class=\"yellowfg\"> 5:50 PM PDT</span>&nbsp;Between 2:40 PM and 5:45 PM PDT, we experienced increased latency and error rates when calling the StartSession API for a subset of customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",
      "service": "ec2systemsmanager-us-west-2"
    },
    {
      "service_name": "Amazon API Gateway (Oregon)",
      "summary": "[RESOLVED] Increased Invoke Error Rates",
      "date": "1664385180",
      "status": "3",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates for invokes in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">10:33 AM PDT</span>&nbsp;We are investigating increased error rates for invokes in the US-WEST-2 Region. We do not yet have a root cause, but are investigating multiple potential root causes in parallel. In addition, we are implementing filters on inbound traffic from a set of sources with recent significant traffic shifts, which may help mitigate the impact.  We do not yet have a solid ETA, but will continue to provide updates as we progress.</div><div><span class=\"yellowfg\">10:59 AM PDT</span>&nbsp;We continue to see elevated error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. While engineers continue to work towards root cause, we have deployed traffic filters from sources with significant increases in traffic prior to the event. As a result of these traffic filters, we are seeing a reduction in error rates and latencies, but continue to work towards full recovery. Although error rates are improving, we do not yet have an ETA for full recovery. The issue is also affecting API requests to some AWS services, including those listed below. Amazon Connect is experiencing increased failures in handling new calls, chats, and tasks as well as issues with user login in the US-WEST-2 Region. We will continue to provide updates as we progress.</div><div><span class=\"yellowfg\">11:33 AM PDT</span>&nbsp;We continue to work on resolving the elevated error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. We continue to see a significant improvement in error rates, starting at 10:40 AM PDT, but are not seeing full recovery yet. The issue is caused by contention within the subsystem that is responsible for request processing within the API Gateway service. Engineers are engaged and have applied traffic filters as a precautionary measure, while they work to identify the root cause and resolve the issue. Engineers continue to work to reduce contention within the affected subsystem, which we believe will resolve the elevated error rates and latencies. Customers with applications that use API Gateway, or customers invoking Lambda functions via API Gateway, will be experiencing elevated error rates and latencies as a result of this issue. The AWS services listed below are also experiencing elevated error rates as a result of this issue. While we have seen improvements in error rates since 10:40 AM PDT, recovery has stalled and we do not have a clear ETA on full recovery. For customers that have dependencies on API Gateway and are experiencing error rates, we do not have any mitigations to recommend to address the issue on the customer side. We do expect error rates to continue to improve as contention with the affected subsystem resides, and will provide further updates as recovery progresses. </div><div><span class=\"yellowfg\">12:26 PM PDT</span>&nbsp;We continue to see an improvement in error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region, but have not fully resolved the issue. While our mitigations have improved error rates and latencies, we have also identified the root cause of the event. The subsystem responsible for request processing experienced increased load, which ultimately led to contention of a component within the affected subsystem. Engineers have been working to resolve the contention of the affected component, which has led to a reduction of error rates and latencies. The path to full recovery involves addressing the contention across the subsystem, which we are currently doing. As that progresses over the next two hours, we expect recovery to continue to improve. Customers with applications that use API Gateway will be experiencing elevated error rates and latencies as a result of this issue. Lambda is not affected by this event, but customers using API Gateway as an HTTP endpoint for Lambda will experience increased error rates and latencies. Other AWS services listed below are also experiencing elevated error rates as a result of this issue. For customers that have dependencies on API Gateway and are experiencing error rates, we do not have any mitigations to recommend to address the issue on the customer side. We do expect error rates to continue to improve as contention with the affected subsystem resides, and will provide further updates as recovery progresses. </div><div><span class=\"yellowfg\"> 1:03 PM PDT</span>&nbsp;Error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region, continue to hold steady. Engineers continue to work on resolving the contention affecting the subsystem responsible for request processing. We recently completed a mitigation that should help to reduce error rates and latencies to normal levels and will have further updates on the result of that change in the next update. Although Lambda function invocations are not affected by this issue, the Lambda console is experiencing some error rates which we are investigating. Other AWS services affected by this issue remain in much the same state, waiting on the recovery of API Gateway.</div><div><span class=\"yellowfg\"> 1:22 PM PDT</span>&nbsp;Starting at 1:12 PM PDT, we saw a further reduction in error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region. This was a result of the latest mitigation which addressed contention within the component in the subsystem responsible for request processing within API Gateway. Error rates are now at levels where some customers may begin to see recovery, and retries will begin to work more consistently. We will be applying the mitigation to the remaining hosts affected by the contention issue and expect to see further recovery from them in the next 30 minutes.</div><div><span class=\"yellowfg\"> 1:42 PM PDT</span>&nbsp;Starting at 1:31 PM PDT, error rates and latencies for invokes on API Gateway endpoints in the US-WEST-2 Region are now close to pre-event levels, and we continue to work on the remaining hosts that are affected by the contention issue. Several AWS services, including AWS Connect and Lambda are seeing signs of strong recovery. We expect all services to recover as API Gateway error rates and latencies return to normal levels. Customers should be seeing recovery at these error levels as well. We will continue to provide updates until the error rates and latencies have returned to normal levels.</div><div><span class=\"yellowfg\"> 2:05 PM PDT</span>&nbsp;As of 1:43 PM PDT, error rates and latencies for invokes for API Gateway endpoints in the US-WEST-2 Region are now at normal levels. The issue began at 9:20 AM PDT when error rates and latencies for API Gateway began to increase. Error rates began to improve at 10:38 AM PDT, when engineers took action to reduce contention within the subsystem that handles request processing for API Gateway. Error rates continued to improve until 1:10 PM PDT, when engineers applied a mitigation to resolve the contention within the affected subsystem. These actions accelerated recovery, and by 1:43 PM PDT, error rates and latencies had returned to normal levels. Affected AWS services have now recovered as well. The issue has been resolved and the service is operating normally.</div>",
      "service": "apigateway-us-west-2"
    },
    {
      "service_name": "Amazon Redshift (N. Virginia)",
      "summary": "[RESOLVED] Increased API errors",
      "date": "1664397551",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:39 PM PDT</span>&nbsp;We are experiencing elevated errors for calls to the Redshift API and console.  We have identified the root cause of these errors, and are in the process of applying a mitigation to address the issue.  While we work to apply the mitigation, customers are encouraged to re-attempt failed API queries, as they are likely to succeed after multiple attempts.  We do not have a firm ETA for recovery at this time, but will report on the results of the mitigation when it has been applied in 1 hour.</div><div><span class=\"yellowfg\"> 2:42 PM PDT</span>&nbsp;Between 6:15 AM and 2:02 PM PDT, we experienced increased latency and error rates in US-EAST-1 Region impacting customers using Query Editor v2 and the Redshift Console. The issue has been resolved and the service is operating normally.</div>",
      "service": "redshift-us-east-1"
    },
    {
      "service_name": "Multiple services (Sao Paulo)",
      "summary": "[RESOLVED] Increased Error rates and Latencies",
      "date": "1664457641",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 6:20 AM PDT</span>&nbsp;We are investigating increased error rates and latencies affecting multiple services and the AWS Console in the SA-EAST-1 Region.</div><div><span class=\"yellowfg\"> 6:39 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for some services within the SA-EAST-1 Region. The AWS Management Console is also experiencing elevated error rates as a result of this issue. The root cause of the issue appears to be elevated network packet loss which is affecting some AWS service APIs. The EC2 network, and existing EC2 instances, are not affected by this event. Some services have seen improvements in error rates as a result of mitigations taken by engineers, but we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 6:55 AM PDT</span>&nbsp;Between 5:56 AM and 6:46 AM PDT, we experienced increased error rates and latencies affecting multiple services and the AWS Management Console in the SA-EAST-1 Region. The EC2 network, and existing EC2 instances, were not affected by this event. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-sa-east-1"
    },
    {
      "service_name": "Amazon Managed Grafana (Oregon)",
      "summary": "[RESOLVED] Authentication Issue",
      "date": "1664954146",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:15 AM PDT</span>&nbsp;We are investigating authentication issues affecting workspace availability in the US-WEST-2 Region</div><div><span class=\"yellowfg\">12:50 AM PDT</span>&nbsp;We have identified the cause of the authentication issue affecting some workspaces in the US-WEST-2 Region and continue to work toward resolution. </div><div><span class=\"yellowfg\"> 1:30 AM PDT</span>&nbsp;Between October 4 11:00 PM and October 5 1:07 AM PDT, we experienced authentication issue affecting some workspaces in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "grafana-us-west-2"
    },
    {
      "service_name": "Multiple services (UAE)",
      "summary": "[RESOLVED] Elevated API Error rates",
      "date": "1665092370",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are investigating elevated error rates for some services in the UAE (ME-CENTRAL-1) Region including S3, Lambda, and CloudTrail.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;Between 2:13 PM PDT and 2:57 PM PDT we experienced increased error rates for Amazon S3 requests in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail. The issue has been resolved and the service is operating normally.</div>",
      "service": "multipleservices-me-central-1"
    },
    {
      "service_name": "Amazon Simple Storage Service (UAE)",
      "summary": "[RESOLVED] Elevated API Error rates",
      "date": "1665093648",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 3:00 PM PDT</span>&nbsp;Beginning at 2:13 PM PDT, we began observing elevated error rates for Amazon S3 PUT and GET APIs in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail.</div><div><span class=\"yellowfg\"> 3:03 PM PDT</span>&nbsp;We are seeing recovery in the elevated error rates for S3 and other AWS services. We are continuing to monitor progress towards full recovery.</div><div><span class=\"yellowfg\"> 3:24 PM PDT</span>&nbsp;Between 2:13 PM PDT and 2:57 PM PDT we experienced increased error rates for Amazon S3 requests in the ME-CENTRAL-1 Region. These error rates also affected other AWS services including Lambda and CloudTrail. The issue has been resolved and the service is operating normally.</div>",
      "service": "s3-me-central-1"
    },
    {
      "service_name": "Amazon Route 53",
      "summary": "[RESOLVED] Increased API error rates and latency ",
      "date": "1665511560",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:06 AM PDT</span>&nbsp;We are investigating increased API error rates and latency in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:16 AM PDT</span>&nbsp;Between 10:33 AM and 11:08 AM PDT, customers may have experienced elevated latency and errors when accessing AWS APIs and consoles in the US-GOV-WEST-1 Region.  We have identified the root cause, which has been addressed, and all services are now operating normally.</div>",
      "service": "route53"
    },
    {
      "service_name": "Amazon Route 53 (US-West)",
      "summary": "[RESOLVED] Increased API error rates and latency ",
      "date": "1665513240",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">11:34 AM PDT</span>&nbsp;Between 10:33 AM and 11:08 AM PDT, customers may have experienced elevated latency and errors when accessing AWS APIs and consoles in the US-GOV-WEST-1 Region. We have identified the root cause, which has been addressed, and all services are now operating normally.  This issue also affected the following services: EC2, Redshift, CloudWatch and OpenSearch.</div>",
      "service": "route53-us-gov-west-1"
    },
    {
      "service_name": "AWS Elastic Beanstalk (N. Virginia)",
      "summary": "[RESOLVED] Increased latency for UpdateEnvironment API calls",
      "date": "1665603967",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are investigating increased latency for UpdateEnvironment API calls. This issue impacts both the API, and requests made through the Management Console. Additionally, attempts to abort the update (via the AbortEnvironmentUpdate API) may return 5XX errors. We are actively working on identifying the root cause and will provide additional information as soon as possible.</div><div><span class=\"yellowfg\"> 1:28 PM PDT</span>&nbsp;We have identified the root cause of increased latency for UpdateEnvironment API calls in the US-EAST-1 Region. This issue impacts both the API, and requests made through the Management Console. Additionally, attempts to abort the update (via the AbortEnvironmentUpdate API) may return a 5XX error. We are observing partial recovery and continue to work through full recovery of the issue. Another update will be sent by 2:00 PM PDT or as more information becomes available.</div><div><span class=\"yellowfg\"> 1:49 PM PDT</span>&nbsp;Between 10:00 AM and 1:45 PM PDT, we experienced increased latency for UpdateEnvironment API calls in the US-EAST-1 Region.  Existing environments were not impacted.  Impact was limited to updating existing or creating new environments. The issue was related to a partially failed deployment that we were able to mitigate. The issue has been resolved and the service is operating normally.  </div>",
      "service": "elasticbeanstalk-us-east-1"
    },
    {
      "service_name": "Amazon Cognito (Ohio)",
      "summary": "[RESOLVED] Increased error rates for Cognito Hosted UI",
      "date": "1666632939",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">10:35 AM PDT</span>&nbsp;Between 8:48 AM and 9:52 AM PDT, we experienced increased error rates for Cognito Hosted UI in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "cognito-us-east-2"
    },
    {
      "service_name": "Amazon Simple Notification Service (Milan)",
      "summary": "[RESOLVED] Increased Error Rates",
      "date": "1666912855",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 4:20 PM PDT</span>&nbsp;We are investigating increased error rates for the Amazon SNS Publish API in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 4:32 PM PDT</span>&nbsp;We are beginning to see recovery in SNS Publish API error rates in the EU-SOUTH-1 Region. </div><div><span class=\"yellowfg\"> 4:55 PM PDT</span>&nbsp;Between 2:57 PM and 4:15 PM PDT we experienced elevated Publish API error rates and delivery latency in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",
      "service": "sns-eu-south-1"
    },
    {
      "service_name": "Amazon Elastic Compute Cloud (Ohio)",
      "summary": "[RESOLVED] Increased Launch Failures",
      "date": "1667246817",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\"> 1:06 PM PDT</span>&nbsp;We are investigating increased failures for newly launched instances within the US-EAST-2 Region. Changes to instance networking state, such as mapping Elastic IP addresses, may also experience increased error rates. We have identified root cause and are working to resolve the issue. Existing instances are not affected by this issue.</div><div><span class=\"yellowfg\"> 1:27 PM PDT</span>&nbsp;Between 12:18 PM and 1:20 PM PDT, we experienced increased failures for newly launched instances within the US-EAST-2 Region. Some APIs, such as those used to attach Elastic IP addresses to instances, were also affected by this event. Some AWS services, such as Elastic Load Balancing and Elastic Map Reduce, experienced provisioning delays due to failures in launching new EC2 instances. Existing instances were not affected by this issue. The issue has been resolved and the service is operating normally.</div>",
      "service": "ec2-us-east-2"
    },
    {
      "service_name": "AWS Transfer for SFTP (N. Virginia)",
      "summary": "[RESOLVED] Increased Authentication Errors",
      "date": "1667939280",
      "status": "1",
      "details": "",
      "description": "<div><span class=\"yellowfg\">12:28 PM PST</span>&nbsp;We can confirm increased authentication errors when logging into SFTP and FTPS. We have identified the root cause and are in the process of mitigating the issue. Our current mitigation efforts are expected to take 90 minutes. We will provide you with another update by 2:00 PM PST.</div><div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;Between 11:10 AM and 12:40 PM PST, we experienced increased authentication errors when logging into SFTP and FTPS. The issue has been resolved and the service is operating normally.</div>",
      "service": "transfer-us-east-1"
    }
  ],
  "current": []
}
